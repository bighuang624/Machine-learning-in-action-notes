## 决策树

### 原理

从数据集中提取一系列规则。

![decision-tree](https://raw.githubusercontent.com/bighuang624/pic-repo/master/decision-tree.png)

### 实现思路

1. 找到当前数据集上在划分数据分类时起决定性作用的特征；
2. 根据该特征，原始数据集被划分为多个数据子集；
3. 如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程，直到所有具有相同类型的数据均在一个数据子集内。

决策树构造算法包括 ID3、C4.5、CART 等。本章讲述的 ID3 用信息论中的熵来度量决策树的决策选择过程，它**无法直接处理数值型数据**（可以通过量化的方法将数值型数据转化为标称型数据）。

#### 信息增益

划分数据集的大原则是：将无序的数据变得更加有序。组织杂乱数据的一种方法就是**使用信息论度量信息**。在划分数据集前后信息发生的变化称为**信息增益(information gain)**，获得信息增益最高的特征就是最好的选择。

集合信息的度量方式成为**香农熵**或简称为**熵（entropy）**，定义为信息的期望值。如果待分类的事物可能被划分在多个分类之中，则符号 $x\_i$的定义为

$$l(x\_i) = -log\_2p(x\_i)$$

其中 $p(x\_i)$是选择该分类的概率。则所有类别所有可能值包含的信息期望值为

$$H = -\sum^n\_{i=1}p(x\_i)log\_2p(x\_i)$$

其中 n 是分类的数目。

计算给定数据集的香农熵的示例代码如下：

```py
# 计算给定数据集的香农熵
from math import log

def calcShannonEnt(dataSet):
    numEntries = len(dataSet)
    labelCounts = {}
    # 为所有可能分类创建字典
    for featVec in dataSet:
        currentLabel = featVec[-1]
        if currentLabel not in labelCounts.keys():
            labelCounts[currentLabel] = 0
        labelCounts[currentLabel] += 1
    shannonEnt = 0.0
    for key in labelCounts:
        prob = float(labelCounts[key]) / numEntries
        shannonEnt -= prob * log(prob, 2)
    return shannonEnt
```

另一个度量无序程度的方法是**基尼不纯度（Gini impurity）**，简单来说就是从一个数据集中随机选取子项，度量其被错误分类到其他分组里的概率。

#### 划分数据集

对每个特征划分数据集的结果计算一次信息熵，然后判断哪个特征划分数据集是最好的划分方式。

```py
# 按照给定特征划分数据集
def splitDataSet(dataSet, axis, value):
    retDataSet = []
    for featVec in dataSet:
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]
            reducedFeatVec.extend(featVec[axis+1:])
            retDataSet.append(reducedFeatVec)
    return retDataSet
```

```py
# 选择最好的数据集划分方式
def chooseBestFeatureToSplit(dataSet):
    numFeatures = len(dataSet[0]) - 1
    baseEntropy = calcShannonEnt(dataSet)
    bestInfoGain = 0.0
    bestFeature = -1
    for i in range(numFeatures):
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)
        newEntropy = 0.0
        # 计算每种划分方式的信息熵
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)
            prob = len(subDataSet) / float(len(dataSet))
            newEntropy += prob * calcShannonEnt(subDataSet)
        infoGain = baseEntropy - newEntropy
        if(infoGain > bestInfoGain):
            bestInfoGain = infoGain
            bestFeature = i
    return bestFeature
```

#### 递归构建决策树

递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。

如果数据集已经处理了所有属性，但是类标签依然不是唯一的，此时我们需要决定如何定义该叶子结点。一般会采取多数表决法决定。

```py
# 创建树的函数代码
def createTree(dataSet, labels):
    classList = [example[-1] for example in dataSet]
    # 类别完全相同则停止继续划分
    if classList.count(classList[0]) == len(classList):
        return classList[0]
    # 遍历完所有特征时返回出现次数最多的
    if len(dataSet[0]) == 1:
        return majorityCnt(classList)
    bestFeat = chooseBestFeatureToSplit(dataSet)
    bestFeatLabel = labels[bestFeat]
    myTree = {bestFeatLabel: {}}
    del(labels[bestFeat])
    featValues = [example[bestFeat] for example in dataSet]
    uniqueVals = set(featValues)
    for value in uniqueVals:
        subLabels = labels[:]
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)
    return myTree
```

### 补充知识

#### Python 中 append 和 extend 的区别

* 使用`append()`的时候，是将参数看作一个对象，整体打包添加到主体对象中。
* 使用`extend()`的时候，是将参数看作一个序列，将这个序列和主体序列合并，并放在其后面。

#### 持久化

与 k-NN 不同，构造好的决策树模型可以通过 Python 内置的持久化模块 pickle 保存在硬盘上。需要使用时可以再读取出来。

```py
# 使用 pickle 模块存储决策树
def storeTree(inputTree, filename):
    import pickle
    fw = open(filename, 'w')
    pickle.dump(inputTree, fw)
    fw.close()

# 读取存储的决策树
def grabTree(filename):
    import pickle
    fr = open(filename)
    return pickle.load(fr)
```

### 总结

ID3 算法：

* 优点：计算复杂度不高，**输出结果易于理解**，可以处理不相关特征数据
* 缺点：没有考虑过拟合的问题，连续特征值无法使用，对于缺失值的情况没有做考虑，用信息增益作为标准容易偏向取值较多的特征
* 适用数据范围：数值型和标称型

### 本节其他参考资料

* [决策树算法原理(上) - 刘建平Pinard - 博客园](http://www.cnblogs.com/pinard/p/6050306.html)

<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
   tex2jax: {inlineMath: [ ['$', '$'] ],
         displayMath: [ ['$$', '$$']]}
 });
</script>

<script src="https://cdn.bootcss.com/mathjax/2.7.4/latest.js?config=default"></script>